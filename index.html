<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blank Page</title>
    <script type="module">
        import { RealtimeClient } from 'https://cdn.jsdelivr.net/gh/openai/openai-realtime-api-beta/index.js';
    </script>

    <style>
        .turn {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            margin-bottom: 20px;
        }
        .conversation {
            background-color: #f9f9f9;
            padding: 10px;
            border-radius: 5px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 15px;
            font-size: 0.9rem;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 4px;
            text-align: center;
        }
        thead {
            background-color: #f2f2f2;
        }
        tfoot {
            font-weight: bold;
            background-color: #e6e6e6;
        }
    </style>
</head>
<body>
  <div style="position: sticky; top: 0; left: 0; width: 100%; background: white; padding-bottom: 10px;">
    <div>
        <label for="apiKey">Enter your API key:</label>
        <input type="password" id="apiKey" placeholder="sk-...">
        <label for="numTurns">Number of turns:</label>
        <input type="number" id="numTurns" min="1" value="6">
    </div>
      <div style="margin-top: 10px; display: flex; flex-direction: row; gap: 10px; width: 100%; justify-content: space-between;"> 
        <button id="runDefaultButton">Run Default</button>
        <button id="runTruncatedButton">Run Truncated</button>
      </div>
    </div>
      <div style="display: flex; flex-direction: row; gap: 10px; width: 100%; height: 100%; overflow-y: auto;">
        <div style="width: 50%;" id="default-results"></div>
      <div style="width: 50%;" id="truncated-results"></div>
    </div>
    
    

    <script type="module">
      import { RealtimeClient } from 'https://cdn.jsdelivr.net/gh/openai/openai-realtime-api-beta/index.js';
      // Converts Float32Array of audio data to PCM16 ArrayBuffer
      function floatTo16BitPCM(float32Array) {
        const buffer = new ArrayBuffer(float32Array.length * 2);
        const view = new DataView(buffer);
        let offset = 0;
        for (let i = 0; i < float32Array.length; i++, offset += 2) {
          let s = Math.max(-1, Math.min(1, float32Array[i]));
          view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
        }
        return buffer;
      }

      // Converts a Float32Array to base64-encoded PCM16 data
      function base64EncodeAudio(float32Array) {
        const arrayBuffer = floatTo16BitPCM(float32Array);
        let binary = '';
        let bytes = new Uint8Array(arrayBuffer);
        const chunkSize = 0x8000; // 32KB chunk size
        for (let i = 0; i < bytes.length; i += chunkSize) {
          let chunk = bytes.subarray(i, i + chunkSize);
          binary += String.fromCharCode.apply(null, chunk);
        }
        return btoa(binary);
      }

      async function fetchAndEncodeWav(url) {
        try {
          const response = await fetch(url);
          const arrayBuffer = await response.arrayBuffer();
          // return arrayBuffer;
          const audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 24000 });
          const audioBuffer = await audioContext.decodeAudioData(arrayBuffer)
          const channelData = audioBuffer.getChannelData(0); // only accepts mono
          const base64AudioData = base64EncodeAudio(channelData);
          return base64AudioData;
        } catch (error) {
          console.error('Error fetching or encoding WAV file:', error);
          return null;
        }
      }

      function calculateCost(event, usage) {
        const u = event.response.usage;
        const input_audio_tokens = u.input_token_details.audio_tokens;
        const output_audio_tokens = u.output_token_details.audio_tokens;
        const input_text_tokens = u.input_token_details.text_tokens;
        const output_text_tokens = u.output_token_details.text_tokens;

        const currentCost = input_audio_tokens * 100 / 1000000 + 
          output_audio_tokens * 200 / 1000000 + 
          input_text_tokens * 5 / 1000000 + 
          output_text_tokens * 20 / 1000000;

        usage.output_audio_tokens += output_audio_tokens;
        usage.input_audio_tokens += input_audio_tokens;
        usage.output_text_tokens += output_text_tokens;
        usage.input_text_tokens += input_text_tokens;
        usage.cost += currentCost;
        usage.currentCost = currentCost;
        return usage;
      }

      const audioSamples = [
        '1-new-york.mp3',
        '2-san-francisco.mp3',
        '3-london.mp3',
        '4-toronto.mp3',
        '5-istanbul.mp3',
      ]

      function printTurn({ turn, conversation, usage, event, testType }) {
        const usageEl = document.createElement('div');
        usageEl.className = 'turn';
        usageEl.innerHTML = `
          <h3>Turn ${turn}</h3>
          <div class="conversation">${conversation.slice(-2).map(c => 
            `<p><strong>${c.role === 'user' ? 'User' : 'Assistant'}:</strong> ${c.text}</p>`).join('')}
          </div>
          <table>
            <thead>
              <tr>
                <th>Tokens</th>
                <th>In Audio</th>
                <th>Out Audio</th>
                <th>In Text</th>
                <th>Out Text</th>
                <th>Cost</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Current</td>
              <td>${event.response.usage.input_token_details.audio_tokens}</td>
              <td>${event.response.usage.output_token_details.audio_tokens}</td>
              <td>${event.response.usage.input_token_details.text_tokens}</td>
              <td>${event.response.usage.output_token_details.text_tokens}</td>
                <td>$${parseFloat(usage.currentCost).toFixed(2)}</td>
              </tr>
            </tbody>
            <tfoot>
              <tr>
                <td>Total</td>
              <td>${usage.input_audio_tokens}</td>
              <td>${usage.output_audio_tokens}</td>
              <td>${usage.input_text_tokens}</td>
              <td>${usage.output_text_tokens}</td>
              <td>$${parseFloat(usage.cost).toFixed(2)}</td>
            </tr>
            </tfoot>
          </table>
        `;
        document.getElementById(`${testType}-results`).appendChild(usageEl);
      }

      const runTest = async (testType = 'default') => {
            const apiKey = document.getElementById('apiKey').value;
            const numTurns = document.getElementById('numTurns').value;
            if (!apiKey) {
                alert('Please enter your API key');
                return;
            }
            const conversation = [];
            try {
                const client = new RealtimeClient({
                    apiKey: apiKey,
                    model: 'gpt-4o-realtime-preview-2024-10-01',
                    dangerouslyAllowAPIKeyInBrowser: true
                });
                client.updateSession({ 
                  input_audio_transcription: { model: 'whisper-1' },
                  temperature: 0.6
                });
                // const wavUrl = 'http://localhost:3000/test_samples_toronto.mp3';
                await client.connect();
                console.log('Connected to OpenAI Realtime API');
                await client.waitForSessionCreated();
                

                let usage = {
                  output_audio_tokens: 0,
                  input_audio_tokens: 0,
                  output_text_tokens: 0,
                  input_text_tokens: 0,
                  cost: 0,
                };
                let turn = 0;
                async function runNextTurn(url) {
                  const base64Audio = await fetchAndEncodeWav(location.href.replace(/\/$/, '') + '/' + url);

                  client.realtime.send('input_audio_buffer.append', {
                    audio: base64Audio
                  })
                  client.realtime.send('input_audio_buffer.commit')
                  client.createResponse();

                  // await client.waitForNextCompletedItem();
                  // client.realtime.send('response.create')
                  await client.waitForNextCompletedItem();
                  await client.waitForNextCompletedItem();

                }
                const itemsToDelete = [];
                client.on('realtime.event', (r) => {
                  const { event } = r;
                  
                  if(testType === 'truncated') {
                    if(event.type === 'conversation.item.created') {
                      itemsToDelete.push(event.item.id)
                    } else if(event.type === 'response.done') {
                      for(const item of itemsToDelete) {
                        client.deleteItem(item);
                      }
                      client.updateSession({
                        instructions: `# Previous conversation\n\n${conversation.map(c => `${c.role === 'user' ? 'User' : 'Assistant'}: ${c.text}`).join('\n')}`
                      })
                    }
                  }

                  if(event.type === 'conversation.item.created') {
                    if(event.item.role === 'user' && event.item.content[0]?.type === 'input_text') {
                      conversation.push({
                        role: 'user',
                        text: event.item.content[0].text
                      })
                    }
                  }
                  else if (event.type === 'conversation.item.input_audio_transcription.completed') {
                    conversation.push({
                      role: 'user',
                      text: event.transcript.replace(/\n$/, '')
                    })
                  }
                  else if (event.type === 'response.output_item.done') {
                    conversation.push({
                      role: 'assistant',
                      text: event.item.content[0].transcript.replace(/\n$/, '')
                    })
                  }
                  else if (event.type === 'response.done') {
                    turn++;
                    usage = calculateCost(event, usage);
                    printTurn({ turn, conversation, usage, event, testType });
                  }
                });

                for(let i = 0; i < numTurns - 1; i++) {
                  await runNextTurn(audioSamples[i % audioSamples.length]);
                }

                const q = 'What was the second city I asked about?'
                client.sendUserMessageContent([
                  { 
                      type: `input_text`,
                      text: q,
                  },
                ]);
                await client.waitForNextCompletedItem();
                await client.waitForNextCompletedItem();
                // wait so we make sure we read the response.done to print
                await new Promise(resolve => setTimeout(resolve, 1000));
                await client.disconnect();
                const resultEl = document.createElement('h3');
                resultEl.innerHTML = `
                  Total Cost: $${parseFloat(usage.cost).toFixed(2)}
                `;
                document.getElementById(`${testType}-results`).appendChild(resultEl);
            } catch (error) {
                console.error('Error:', error);
            }
    
        };
        document.getElementById('runDefaultButton').addEventListener('click', () => runTest('default'));
        document.getElementById('runTruncatedButton').addEventListener('click', () => runTest('truncated'));
    </script>
</body>
</html>
